{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Scenario: **Credit Card Fraud Detection Using CNN**","metadata":{"id":"mpEEbZ54W2lw"}},{"cell_type":"markdown","source":"","metadata":{"id":"HJhy3dxr4E2p"}},{"cell_type":"markdown","source":"","metadata":{"id":"V3CgIqm2nNO9"}},{"cell_type":"markdown","source":"###**Dataset Description**\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. \n\nPresents transactions that occurred in two days, where we have **492** frauds out of **284,807** transactions. \n\n- **Time** - Number of seconds elapsed between this transaction and the first transaction in the dataset\n- **V1-V28** - Encrpted attributes (or columns) to protect user identities and sensitive features (v1-v28)\n- **Amount** - Transaction Amount\n- **Class** - **1** for fraudulent transactions, **0** otherwise","metadata":{"_uuid":"8acee0a244846f03a1436fb98dc90f6a8a86fdbf","id":"MBjCgFbQyCDT"}},{"cell_type":"markdown","source":"###**Tasks to be performed**\n\n- Import the required libraries and load the dataset\n- Perform Exploratory Data Analysis (EDA) on the data set\n  - Plot **Univariate Distributions**\n    - What is the distribution of the **amount** & **class** columns in the data set?\n    \n- Pre-process that data set for modeling\n  - Handle Missing values present in the data set\n  - Scale the data set using **RobustScaler()**\n  - Split the data into training and testing set using sklearn's **train_test_split** function\n- Build a CNN Model Using Tensorflow 2.0\n- Compile and fit the model\n- Plot the Training History\n      - Make a plot for the Loss Function to visualize the change in Loss at every epoch\n      - Make a plot for the Accuracy Metric to visualize the accuracy at every epoch\n- Build CNN Model 2 with MaxPooling Layers\n- Plot the Training History \n      - Make a plot for the Loss Function to visualize the change in Loss at every epoch\n      - Make a plot for the Accuracy Metric to visualize the accuracy at every epoch\n","metadata":{"id":"8dX5FUCfYp6h"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{"id":"4lKtaaXEYvzY"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","metadata":{"id":"PwBopS7pW8VP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('creditcard.csv')\ndf.head()","metadata":{"id":"EhEqontuW-Xm","outputId":"6c661e94-ce7e-4ae0-c9eb-6c99b0133e99"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Exploratory Data Analysis**","metadata":{"id":"3881bLhnr_7B"}},{"cell_type":"markdown","source":"####**Plotting Univariate Distributions**\nA **Univariate distribution** is a probability distribution of only one random variable\n\n**Note:** You have already seen this in Pandas Profiling. Still, if you want to write the code, you can do so.","metadata":{"id":"1M6pXJhgUPsi"}},{"cell_type":"markdown","source":"What is the distribution of the **Time** & **Amount** columns in the data set?","metadata":{"id":"CJ0EcFjiybU7"}},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.histogram(df, x = 'Time')\nfig.show()","metadata":{"id":"YEmpfR0ZygWS","outputId":"01673142-0bc4-42a8-b57d-7778eac7b81b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, x = 'Amount')\nfig.show()","metadata":{"id":"A0RvYsvvy5fP","outputId":"eefa2f12-7d5a-4140-c7c3-8730151183b9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####**Checking Imbalanced Dataset**","metadata":{"id":"QGT6LFLXIF8M"}},{"cell_type":"code","source":"import seaborn as sns\n%matplotlib inline\n\nplt.figure(figsize=(12,8))\nax = sns.countplot(df[\"Class\"], color='green')\nfor p in ax.patches:\n    x = p.get_bbox().get_points()[:,0]\n    \n    y = p.get_bbox().get_points()[1,1]\n    \n    ax.annotate('{:.2g}%'.format(100.*y/len(df)), (x.mean(), y), ha='center', va='bottom')\nplt.show()","metadata":{"id":"CAz7_w8FAW8l","outputId":"bf132816-ffd0-4ca9-f180-f389e5c3392e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n**Observations:**\n\nThe data set is **Highly Unbalanced** with only **0.17%** of transactions being classified as **Fraudulent**. \n\nSeveral ways to approach this Imbalance Classification problem:\n\n- **Acquire More Data**\n- **Changing the performance metric:**\n - Use the **Confusion Matrix**\n - **F1-Score** (Weighted Average of **Precision** & **Recall**)\n - **ROC Curves**\n\n- **Re-sampling the dataset:** Essentially this is a method that will process the data to have an approximate 50-50 ratio.\n\n - **Over-sampling**, which is adding copies of the under-represented class (better when you have little data)\n\n - **Under-sampling**, which deletes instances from the over-represented class (better when he have lot's of data)\n___","metadata":{"id":"LiFoAs4LDXr4"}},{"cell_type":"markdown","source":"###**Data Manipulation**","metadata":{"id":"EK2Zneg2rO2R"}},{"cell_type":"code","source":"#Robust Scaler is similar to normalization but it instead uses the interquartile range, so that it is robust to outliers\n\nfrom sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\n\n#Fit_Transform the scaled_amount and scaled_time columns in the data set and dropping the Original Time and Amount Column from the data set\n\ndf['scaled_amount'] = rs.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rs.fit_transform(df['Time'].values.reshape(-1,1))\ndf.drop(['Time', 'Amount'], axis=1, inplace=True) #Dropping the Original Time and Amount Column from the data set","metadata":{"id":"Rc8xVC7uwAGb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(0, 'scaled_time', scaled_time)\ndf.head()","metadata":{"id":"8q-GA2S1wPb0","outputId":"c2dc2293-9ef8-41b9-a060-2c993205f178"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split as holdout #Importing the train_test_split from sklearn library\nx = np.array(df.iloc[:, df.columns != 'Class']) #Predictors \ny = np.array(df.iloc[:, df.columns == 'Class']) #Target Column\nx_train, x_test, y_train, y_test = holdout(x, y, test_size=0.2, random_state=0) #Splitting the data set into training and testing set","metadata":{"id":"VMf12qcbwrKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"id":"w_kc6TU3qMXK","outputId":"a737a4f1-dd49-4fef-ff2a-3c38ceddd1e1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\nx_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n\nx_train.shape, x_test.shape","metadata":{"id":"6bQCsvG6qPXv","outputId":"2eab1b02-1fa5-4055-dfb1-4acc17369daf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Build a CNN Model Using Tensorflow 2.0**","metadata":{"id":"4Q6kIsD43A63"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv1D, MaxPool1D\n\nmodel = Sequential()\n\nmodel.add(Conv1D(32, 2, activation='relu', input_shape = x_train[0].shape))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(64, 2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1,, activation='sigmoid'))\n#The output layer: Dense layer with 1 neuron. We are predicting a single value as this is a binary classification problem\n#Using Sigmoid function because it exists between (0 to 1) and this facilitates us to predict a binary input","metadata":{"id":"YSnYw-BKXcDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"uBcfJ3lSXe_l","outputId":"402a07ce-d60c-47d7-b457-c26161092820"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Each layer has an output and its shape is shown in the **Output Shape** column\n- Each layerâ€™s output becomes the input for the next layer\n- The **Param #** column shows the number of parameters that are trained for each layer\n- The total number of parameters is shown at the end, which is equal to the number of **trainable** and **non-trainable parameters**","metadata":{"id":"Km5i4_4VI_YT"}},{"cell_type":"markdown","source":"###**Compile and fit the model**","metadata":{"id":"DiX9yuNH3J7c"}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam","metadata":{"id":"KIzf2FDqGKSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model optimizer, loss function and metrics\n\nmodel.compile(\n    optimizer=Adam(lr=0.0003), \n    loss = 'binary_crossentropy', \n    metrics=['accuracy']\n    )","metadata":{"id":"1EuFBUWZXgu3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####**EarlyStopping**\n\nWe will be using Callback to implement a regularisation approach called **Early Stopping**.\n\n**EarlyStopping** is a technique that monitors the performance of the network for every epoch on a held out validation set during the training run and terminates the training based on the Validation Performance.\n\n","metadata":{"id":"bNhtbaxiBkOQ"}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience = 5, min_delta = 0.01, mode='max')\n\n#In this case, the training will terminate only if there is no improvement in the monitor performance measure for 5 epochs in a row\n#0.01 means that the Validation_Accuracy has to improve by atleast 0.01 for it to count as an Improvement\n","metadata":{"id":"yO7Ymy2x_8b3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.fit returns a Tensorflow History Object\n#It contains a record of the progress of the Network during training in terms of the Loss and the Metrics\n\nhistory = model.fit(x_train, y_train, epochs=40, validation_data=(x_test, y_test), verbose=2, callbacks=[early_stopping])","metadata":{"id":"xLmjN5apXitq","outputId":"60b057da-7c2b-4927-a357-ed64a493189e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The History Object is A Dictionary that contains information about Loss Fucntions and Metrics after each of the Epochs\ndf = pd.DataFrame(history.history)\n\ndf\n#This is quite useful to check how the Training is going","metadata":{"id":"Nqb3ibUiB7ae","outputId":"df8ca824-13d2-4088-c82c-9251ef45e52b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Plot Training History** ","metadata":{"id":"qrz1fTWGDrd5"}},{"cell_type":"code","source":"# Make a plot for the loss\n#This Shows how the Loss Function decreases after Every Epoch\n\nloss_plot = df.plot(y = 'loss', title = 'Loss vs Epochs', legend = False)\nloss_plot.set(xlabel = 'Epochs', ylabel = 'Loss')","metadata":{"id":"MR19fPgVCF_c","outputId":"2754e2e3-2138-4149-d6fa-1575f4008a9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a plot for the accuracy\n\nacc_plot = df.plot(y = 'accuracy', title = 'accuracy vs epochs', legend = False)\nacc_plot.set(xlabel = 'epochs', ylabel = 'accuracy')","metadata":{"id":"S5R6ghscCF8k","outputId":"95295d3a-27c2-4fb6-c108-24a59dbc8aef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Add MaxPooling Layers and create a CNN Model 2**","metadata":{"id":"owfzTkqFvvUR"}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv1D(32, 2, activation='relu', input_shape = x_train[0].shape))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(64, 2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation='sigmoid'))\n#The output layer: Dense layer with 1 neuron. We are predicting a single value as this is a binary classification problem\n#Using Sigmoid function because it exists between (0 to 1) and this facilitates us to predict a binary input","metadata":{"id":"-uwXkRtFXqfe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), \n              loss = 'binary_crossentropy', \n              metrics=['accuracy'])","metadata":{"id":"nzb1x_mpEM63"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience = 5, min_delta = 0.01, mode='max')\n\n#In this case, the training will terminate only if there is no improvement in the monitor performance measure for 5 epochs in a row\n#0.01 means that the Validation_Accuracy has to improve by atleast 0.01 for it to count as an Improvement\n","metadata":{"id":"K74xxeYXEmnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train, epochs=80, validation_data=(x_test, y_test), verbose=2, callbacks=[early_stopping])","metadata":{"id":"35_pNG_4EM22","outputId":"221c2b70-18b6-4577-ffbe-d22e88065ca0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The History Object is A Dictionary that contains information about Loss Fucntions and Metrics after each of the Epochs\ndf = pd.DataFrame(history.history)\n\ndf\n#This is quite useful to check how the Training is going","metadata":{"id":"Kvd84AkvEMzu","outputId":"d5b5932a-db05-487e-a4be-352ffeef2ace"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Plot the training history**\n","metadata":{"id":"5vFNeEI43krB"}},{"cell_type":"code","source":"# Make a plot for the loss\n#This Shows how the Loss Function decreases after Every Epoch\n\nloss_plot = df.plot(y = 'loss', title = 'Loss vs Epochs', legend = False)\nloss_plot.set(xlabel = 'Epochs', ylabel = 'Loss')","metadata":{"id":"PxVCD0USXs56","outputId":"64953cb0-c487-41ca-da62-8c0925393a95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a plot for the accuracy\n\nacc_plot = df.plot(y = 'accuracy', title = 'accuracy vs epochs', legend = False)\nacc_plot.set(xlabel = 'epochs', ylabel = 'accuracy')","metadata":{"id":"GvPyoyczXu7q","outputId":"ed4e031d-f788-4d95-fe00-760bfa2eedf5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Benefit of Using CNN for this project:**\n\n- Better accuracy than traditional Machine Learning Algorithms such as SVM, Random Forest, etc\n- Datasets\tavailable\tfor\ttraining\tare\thighly imbalanced,\twith\tthe\tnumber\tof\tfradulent\ttransactions\tconsiderably\tless\tthan the\tother. Model could be biased towards the majority class. Oversampling\tthe\tminority\tclass\tis\tone\tapproach\tto\tmitigate\tthis\tproblem\tbut\tit\tstill\thas\tits\tdrawbacks\n- Using a Deep Learning based CNN model may fare better and it did so in our case\n\n","metadata":{"id":"H4QdNb2J0yeP"}}]}